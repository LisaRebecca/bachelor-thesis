\chapter{Business Understanding}
In the guide complementing the \ac{CRISP-DM} model, different tasks, and outputs for developing a business understanding are mentioned. The task and respective output will be discussed in the following sections.

\section{Business Objectives}
\label{section:business-objectives}
Businesses without existing an \ac{ERP} solution in place can easily be overwhelmed by the number of invoices reaching them daily. Even more, the controlling department can easily lose the overview of spending. To quickly gain a perspective on the most important spending topics, spending should be sorted in categories of similar nature.

The primary goal is the development of a solution for automatic aggregation of documents, based on topics addressed in those documents. The focus is on shorter text segments, such as product descriptions. The business objective is an information gain, on how spending is distributed among cross-cutting topics in a company.

The created solution can be evaluated with the business success criterion: "Does the solution identify and give useful insights in the money pits?". The judgment of goal achievement is a subjective matter. Evaluating the success should therefore be distributed among several stakeholders, including but not limited to the author, the supervisor and the supplier of the data.

\section{Assessment of the Situation}

\subsection{Inventory of Resources}
An inventory of resources (\ref{tabelle:inventory}) was created for assessing the situation. Most notably is the availability of experts through excellent inter-corporation cooperation. Also, a large collection of datasets is available. Hardware platforms include personal machines as well as hosted environments with GPU capabilities. Available software are data science tools included in the Anaconda Navigator, such as Jupyter Notebook. All open-source libraries are of course also included.

\subsection{Requirements, Assumptions, and Constraints}
The project is to be completed the latest on June 7$^{th}$ 2022. 

Several assumptions underlay the process of data mining. First, it is assumed that the descriptions of the invoices is speaking enough to identify the product referenced.
Second, the analysis assumes that invoices can be logically grouped into clusters, in other words, several invoices referring to similar topics.

From a legal perspective, the project is constrained in the publication of data. While the use and processing of the supplied data is permitted within the context of the thesis, publication and further use is prohibited. The dataset is to be kept only on the local machine and SAP owned hyperscaler instances.

\subsection{Risks and Contingencies}
\paragraph{Dataset too Large for Processing} One specific risk that may arise in this project, is a dataset too large to be processed. Only a limited size of data is able to be loaded into memory. If the dataset turns out to be too big, four solutions are proposed.

\begin{enumerate}
	\item The dataset can be compressed using either a lossy compression (sampling, truncating floating point values) or a lossless compression (choosing only specific columns, using a sparse-column representation or choosing efficient data types) \cite{largeDataSetMedium}.
	\item Memory problems often arise out of computations that are not thought through. Most of the time, not the whole data needs to be in memory. Streaming the data or loading it progressively is a great option, if the algorithms permit this \cite{largeDataSetBrownlee}. Programming languages often have built in lazy evaluation capabilities, such as Python's generators.
	\item Another approach for storing and querying large datasets is the use of a relational database. The database can be queried using \ac{SQL}. Again, this option has the premise of \ac{ML} algorithms permitting iterative learning \cite{largeDataSetBrownlee}. 
	\item Finally, using a platform for \ac{ML} workloads, such as SAP AI Core helps with handling large amounts of data. This approach requires aligning of the source code to fit the specifications and endpoints of the platform.
\end{enumerate}

\paragraph{Messy Data} Of course, data collected from operational sources is not perfect and ready to feed into an algorithm. Data cleaning is one of the main activities of data scientist's everyday workload. But what if the dataset is untidy to such a severe extent, that it is not able to be cleaned in a reasonable amount of time?
Particular problems can be column shifts in tables, different units for values without naming the unit, or feature encodings without keys for decoding.
Depending on the extent of the case, a different dataset should be evaluated. Also, the remark has to be made that this case is highly unlikely as the datasets have been used for other applications in the past.

\paragraph{Inefficient Calculation} Calculations can quickly grow into inefficient and obfuscated code, taking hours or even days to complete. To mitigate this risk the following contingencies are proposed:

\begin{enumerate}
	\item Different methods for calculating the same operation should be identified, evaluated and implemented.
	\item Regular bench-marking of smaller chunks of the data helps to extrapolate processing times and decide for one solution.
	\item Implementations in libraries should be, in general, favored over self-made implementation. Literature research in industry-specific blogs helps to find even more efficient implementations than those contained in popular libraries.
	\item A sophisticated design of data structures is crucial in utilizing optimized code. Even the most elaborate way of calculating operations can turn into a resource-intensive task if the input data is structured poorly. Considering different data structures and selecting the most appropriate one for each specific task is crucial.
\end{enumerate}

\paragraph{Results are of no Value}
The business objectives were determined in a prior section. But what is the procedure if the business objectives are not reached? Especially the criterion of giving useful insights is the crux.
A simple laissez-faire attitude towards the definition of "useful" is unsatisfactory, although creating the illusion of a positive outcome of the project.
With not reaching the original goal of a research project, the work does not automatically become useless. Identifying problems and causes for the failure can facilitate other research.

\subsection{Terminology}
To summarize both relevant data mining and business terminology, a glossary was compiled.

	\paragraph{Clustering Algorithm} A clustering algorithm is a sequence of instructions, which arranges a set of instances into groups, which contain items of high similarity to each other.
	\paragraph{Document} In the context of \ac{NLP}, a document is a unit of data containing natural language text. In the context of this thesis, 'document' refers to the product description of one line item in an invoice.
	\paragraph{Hyperparameter} The hyperparameters of a \ac{ML} model is a configured trait of the model. Usually, it is specified by the practitioner and altered over time in a process called hyperparameter tuning \cite{hyperparameter}.
	\paragraph{Hyperscaler} A hyperscaler is a company offering architecture to adapt to changing workloads in cloud computing, networking and internet services.
	\paragraph{\acf{NLP}} \ac{NLP} is a discipline comprised of linguistics, computer science, artificial intelligence an mathematics \cite[p.~51]{chowdhury2003}.
	\paragraph{Overfitting} Overfitting is a problem occurring in machine learning, in which the model adapts too closely to the data and is not able to make meaningful inferences on new data.
	\paragraph{Pickle} The process of pickling is a form of serialization in Python.
	\paragraph{Script} A script is  a small piece of code contained in a single file, most commonly written in a dynamic high-level programming language.
	\paragraph{SAP AI Foundation} The SAP AI Foundation is a term describing SAP's offerings of the AI Launchpad and AI Core.
	\paragraph{SAP AI Launchpad} The SAP AI Launchpad is a tool for the operation of AI content inside an SAP system.
	\paragraph{SAP AI Core} SAP AI Core allows for training and serving AI scenarios \cite{schmitzLeonardo}.
	\paragraph{Token} A token is the "atomic unit" \cite{MLGlossary} of a natural language model. Tokens can be whole words, characters or parts of a word.
	\paragraph{Underfitting} The counterpart to overfitting is underfitting. In this problem, the \ac{ML} model is not complex enough to fully capture the complexity of the data \cite{MLGlossary}.


\section{Data Mining Goals}
\label{section:data-mining-goals}
The following data mining goals were identified during the phase of business understanding:

\begin{enumerate}
\item Identifying and applying appropriate methods for feature extracting tailored to this type of dataset.
\item Identifying and applying appropriate methods for clustering documents in this type of dataset.
\item Identifying and applying appropriate methods for topic modeling with this type of dataset.
\item Aggregating expenses by their clusters and visualizing the output.
\end{enumerate}

The successful outcome is defined by reaching all named criteria. The achievement will be evaluated by the author and the supervisor, also people referenced in the inventory of resources will be considered to evaluate the outcome.


\section{Initial assessment of tools and techniques}

\paragraph{Programming Language}
A multitude of programming languages for statistical analyses and machine learning applications exist. The most popular and best supported are Python and R. Both languages are open source and targeted at data science tasks.
Following criteria will be used to evaluate the best fit for this project:
\begin{enumerate}
\item Understanding, as the availability of learning resources and the ease of reading and writing source code.
\item Availability of resources such as libraries, packages and modules.
\item Compatibility with existing solutions, availability of scaling and deployment options.
\end{enumerate}

Python is a general purpose programming language with a straightforward syntax. Python is regularly taught at schools and suited for beginners. R is a language built by statisticians. Therefore, R is suited primarily for data-science tasks. While a data analysis can be created easily, more complex functionality requires experience and is considered challenging \cite{pythonVsR}. For understanding, Python is rated as the more favorable choice.

For the availability of resources, R clearly scores. In general, Python has a large number of libraries available. Since Python is not exclusively for data science, only a fraction of them are suited for statistical analyses. For R on the other hand, over 13.000 packages are available in the R archives \cite{pythonVsR}.

Deploying a Python solution is easily possible via APIs, web apps and containerization technologies. R is also displayable on the web using specific packages. Both R and Python can be deployed in docker containers \cite{rDocker} \cite{pythonDocker} paving the way to be deployed on all major hyperscalers.
It can be stated that Python has superior ability to be scaled and deployed as it is a general purpose programming language. 

\paragraph{Programming Paradigm}
With the programming language decided, the basic programming paradigm has to be chosen. Python is an high-level object-oriented language, containing also functional aspects \cite[ch.~1.3.2]{corePython}. Python allows for programming using only scripts, but also supports and encourages the use of modules to separate files.
For data scientist, the decision between modular code and code contained in a notebook presents itself in the beginning of every project. Python code in modules can be organized into several files calling each other when needed. This option requires careful documentation, as there is no graphical interface showing dependencies between the modules.

Python notebooks are structured to represent code, descriptions and the output. The most popular notebook format are Jupyter Notebooks, representing code and additional information as \ac{JSON} data \cite{jupyterteramArchitecture}.
While notebooks allow to tell a story using visualizations and descriptions, they lack in ability to be easily deployed using common methods. Of course, the deployment of a Jupyter Notebook is not impossible, but disproportionately more difficult than deploying Python modules.

Since not every part of a data-science project has to be deployed, a hybrid approach to Juypter Notebooks and modular coding makes sense. The data understanding part of the project will be completed using notebooks. The data preparation and the modeling will be completed in Python modules to ensure being able to be deployed, if desired.












