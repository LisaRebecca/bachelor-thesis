
@book{40algorithms,
  title = {40 {{Algorithms Every Programmer Should Know}}: {{Hone}} Your Problem-Solving Skills by Learning Different Algorithms and Their Implementation in {{Python}}},
  author = {Ahmad, I.},
  date = {2020},
  edition = {1},
  publisher = {{Packt Publishing}},
  isbn = {978-1-78980-986-2}
}

@online{AIOverviewResearch,
  title = {{{AI Research}}},
  url = {https://www.sap.com/products/artificial-intelligence/research.html},
  urldate = {2022-04-27}
}

@online{alammarIllustratedBERTELMo2018,
  title = {The {{Illustrated BERT}}, {{ELMo}}, and Co. ({{How NLP Cracked Transfer Learning}})},
  author = {Alammar, Jay},
  date = {2018-12-03},
  url = {https://jalammar.github.io/illustrated-bert/},
  urldate = {2022-05-20},
  abstract = {Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Persian, Russian 2021 Update: I created this brief and highly accessible video intro to BERT The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It’s been referred to as NLP’s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).},
  file = {/Users/I518232/Zotero/storage/G5BTG9YP/illustrated-bert.html}
}

@online{ArchitectureJupyterDocumentationa,
  title = {Architecture — {{Jupyter Documentation}}},
  url = {https://docs.jupyter.org/en/latest/projects/architecture/content-architecture.html},
  urldate = {2022-04-30}
}

@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  doi = {10.48550/ARXIV.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2022-05-19},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/I518232/Zotero/storage/6FFP36QV/Ba et al. - 2016 - Layer Normalization.pdf;/Users/I518232/Zotero/storage/CRVQ9JYQ/1607.html}
}

@misc{BERT,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-05-20},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/I518232/Zotero/storage/KX9HQT5F/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/I518232/Zotero/storage/9KT9MGZN/1810.html}
}

@online{BERTEmbeddingLayer,
  title = {Why {{BERT}} Has 3 {{Embedding Layers}} and {{Their Implementation Details}}},
  author = {\_\_\_},
  date = {2021-01-03T14:36:54},
  url = {https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a},
  urldate = {2022-05-20},
  abstract = {Why does BERT have 3 embedding layers instead of 1 like most deep learning-based NLP models?},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/I518232/Zotero/storage/DBIG2SXZ/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a.html}
}

@online{BERTTraining,
  title = {{{BERT}} 101 - {{State Of The Art NLP Model Explained}}},
  author = {Muller, Britney},
  date = {2022-03-02},
  url = {https://huggingface.co/blog/bert-101},
  urldate = {2022-05-20},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/I518232/Zotero/storage/F67ESUMJ/bert-101.html}
}

@article{beyerNearestNeighbor,
  title = {When {{Is}} “{{Nearest Neighbor}}” {{Meaningful}}?},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  date = {1998},
  journaltitle = {ICDT},
  volume = {1540},
  pages = {217--235},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.236&rep=rep1&type=pdf},
  abstract = {We explore the effect of dimensionality on the “nearest neighbor” problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10-15 dimensions.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/JNTP4SFM/Beyer et al. - When Is “Nearest Neighbor” Meaningful.pdf}
}

@book{buxmannKI2019,
  title = {Künstliche Intelligenz: mit Algorithmen zum wirtschaftlichen Erfolg},
  shorttitle = {Künstliche Intelligenz},
  editor = {Buxmann, Peter and Schmidt, Holger},
  date = {2019},
  publisher = {{Springer Gabler}},
  location = {{Berlin}},
  doi = {10.1007/978-3-662-57568-0},
  abstract = {Dieses Buch soll dabei helfen, die neuen Technologien und Anwendungspotenziale der künstlichen Intelligenz besser zu verstehen und einzuordnen. Neben einer ausführlichen und verständlichen Vermittlung grundlegender Kenntnisse und ökonomischer Effekte der künstlichen Intelligenz enthält es viele Anwendungsbeispiele bekannter Unternehmen. Konzerne wie Amazon, IBM, Microsoft, SAP oder VW lassen die Leser in ihre KI-Labors schauen und erklären konkrete Projekte zu Themen, wie z. B. Chatbots, Quantencomputing, Gesichtserkennung, sprachbasierte Systeme oder den Einsatz von KI-Anwendungen in den Bereichen Marketing, Vertrieb, Finanzen, Personalwesen, Produktion, Gesundheit sowie Logistik. Das Buch richtet sich an Entscheider in Unternehmen, Studierende, Dozenten und alle, die sich ein Bild über die vielleicht wichtigste technologische Entwicklung in diesem Jahrhundert machen möchten.(Verlagstext)},
  isbn = {978-3-662-57568-0 978-3-662-57567-3},
  langid = {german},
  pagetotal = {206},
  file = {/Users/I518232/Zotero/storage/3IKC5KGI/Buxmann and Schmidt - 2019 - Künstliche Intelligenz mit Algorithmen zum wirtsc.pdf}
}

@article{chowdhury2003,
  title = {Natural Language Processing},
  author = {Chowdhury, Gobinda},
  date = {2003-01-31},
  journaltitle = {Annual Review of Information Science and Technology},
  volume = {37},
  number = {1},
  pages = {51--89},
  publisher = {{Springer}},
  issn = {0066-4200},
  doi = {10.1002/aris.1440370103},
  url = {https://strathprints.strath.ac.uk/2611/1/strathprints002611.pdf}
}

@online{Clustering,
  title = {2.3. {{Clustering}}},
  url = {https://scikit-learn/stable/modules/clustering.html},
  urldate = {2022-05-18},
  abstract = {Clustering of unlabeled data can be performed with the module sklearn.cluster. Each clustering algorithm comes in two variants: a class, that implements the fit method to learn the clusters on trai...},
  langid = {english},
  organization = {{scikit-learn}},
  file = {/Users/I518232/Zotero/storage/5NWR9U9L/clustering.html}
}

@book{corePython,
  title = {Core Python Programming},
  author = {Chun, Wesley},
  date = {2006-09},
  volume = {1},
  publisher = {{Prentice Hall Professional}}
}

@misc{CRISPDM2000,
  title = {{{CRISP-DM}} 1.0: {{Step-by-step}} Data Mining Guide},
  author = {Chapman, Peter and Clinton, Janet and Kerber, Randy and Khabaza, Tom and Reinartz, Thomas P. and Shearer, Colin and Wirth, Richard},
  date = {2000},
  publisher = {{SPSS Inc.}},
  file = {/Users/I518232/Zotero/storage/XWX5MT84/Chapman et al. - 2000 - CRISP-DM 1.0 Step-by-step data mining guide.pdf}
}

@online{CRISPDMPoll,
  title = {{{CRISP-DM}}, Still the Top Methodology for Analytics, Data Mining, or Data Science Projects},
  author = {{says}, Esport},
  url = {https://www.kdnuggets.com/crisp-dm-still-the-top-methodology-for-analytics-data-mining-or-data-science-projects.html/},
  urldate = {2022-03-29},
  abstract = {CRISP-DM remains the most popular methodology for analytics, data mining, and data science projects, with 43\% share in latest KDnuggets Poll, but a replacement for unmaintained CRISP-DM is long overdue.},
  langid = {american},
  organization = {{KDnuggets}},
  file = {/Users/I518232/Zotero/storage/HC5XIX9T/crisp-dm-top-methodology-analytics-data-mining-data-science-projects.html}
}

@online{CRISPDMPopular2020,
  title = {{{CRISP-DM}} Is {{Still}} the {{Most Popular Framework}} for {{Executing Data Science Projects}}},
  author = {Saltz, Jeff},
  date = {2020-11-30T01:31:31+00:00},
  url = {https://www.datascience-pm.com/crisp-dm-still-most-popular/},
  urldate = {2022-03-29},
  abstract = {Based on a recent poll conducted on our site, CRISP-DM remains as the most popular framework for data science projects.},
  langid = {american},
  file = {/Users/I518232/Zotero/storage/VCIPKW2Y/crisp-dm-still-most-popular.html}
}

@article{dataQualityAzeroual,
  title = {Data Measurement in Research Information Systems: Metrics for the Evaluation of Data Quality},
  shorttitle = {Data Measurement in Research Information Systems},
  author = {Azeroual, Otmane and Saake, Gunter and Wastl, Jürgen},
  date = {2018-06},
  journaltitle = {Scientometrics},
  shortjournal = {Scientometrics},
  volume = {115},
  number = {3},
  pages = {1271--1290},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-018-2735-5},
  url = {http://link.springer.com/10.1007/s11192-018-2735-5},
  urldate = {2022-05-07},
  abstract = {In recent years, research information systems (RIS) have become an integral part of the university’s IT landscape. At the same time, many universities and research institutions are still working on the implementation of such information systems. Research information systems support institutions in the measurement, documentation, evaluation and communication of research activities. Implementing such integrative systems requires that institutions assure the quality of the information on research activities entered into them. Since many information and data sources are interwoven, these different data sources can have a negative impact on data quality in different research information systems. Because the topic is currently of interest to many institutions, the aim of the present paper is firstly to consider how data quality can be investigated in the context of RIS, and then to explain how various dimensions of data quality described in the literature can be measured in research information systems. Finally, a framework as a process flow according to UML activity diagram notation is developed for monitoring and improvement of the quality of these data; this framework can be implemented by technical personnel in universities and research institutions.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/26I6JWLE/Azeroual et al. - 2018 - Data measurement in research information systems .pdf}
}

@article{dataScienceProjectTypes,
  title = {{{CRISP-DM Twenty Years Later}}: {{From Data Mining Processes}} to {{Data Science Trajectories}}},
  shorttitle = {{{CRISP-DM Twenty Years Later}}},
  author = {Martinez-Plumed, Fernando and Contreras-Ochando, Lidia and Ferri, Cesar and Hernandez-Orallo, Jose and Kull, Meelis and Lachiche, Nicolas and Ramirez-Quintana, Maria Jose and Flach, Peter},
  date = {2021-08-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  volume = {33},
  number = {8},
  pages = {3048--3061},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2019.2962680},
  abstract = {CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/3QPU4C26/Martinez-Plumed et al. - 2021 - CRISP-DM Twenty Years Later From Data Mining Proc.pdf}
}

@article{DBSCAN,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  date = {1996},
  journaltitle = {KDD},
  volume = {96},
  number = {34},
  pages = {226--231},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/GQU8AMLM/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf}
}

@online{DFvsJSON,
  title = {Memory {{Optimisation}} – {{Python DataFrames}} vs {{Lists}} and {{Dictionaries}} ({{JSON-like}})},
  author = {Tok, Joel},
  date = {2021-06-07},
  url = {https://www.joeltok.com/blog/2021-6/memory-optimisation-python-dataframes-vs-json-like},
  urldate = {2022-05-13}
}

@book{epsteinParsingTuringTest2008,
  title = {Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer},
  shorttitle = {Parsing the Turing Test},
  editor = {Epstein, Robert and Roberts, Gary and Beber, Grace},
  date = {2008},
  publisher = {{Springer}},
  location = {{New York}},
  isbn = {978-1-4020-6708-2 978-1-4020-6710-5},
  pagetotal = {517},
  keywords = {Artificial intelligence,Methodology,Philosophy}
}

@article{evaluationTopicModelling,
  title = {An Evaluation of Document Clustering and Topic Modelling in Two Online Social Networks: {{Twitter}} and {{Reddit}}},
  author = {Curiskis, Stephan and Drake, Barry and Osborn, Thomas and Kennedy, Paul},
  date = {2019-04},
  journaltitle = {Information Processing \& Management},
  volume = {57},
  doi = {10.1016/j.ipm.2019.04.002}
}

@online{externalData,
  title = {Harnessing the Power of External Data | {{McKinsey}}},
  url = {https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/harnessing-the-power-of-external-data},
  urldate = {2022-04-06},
  file = {/Users/I518232/Zotero/storage/QUMXN8XD/harnessing-the-power-of-external-data.html}
}

@online{GeschichteSAP1972,
  title = {Geschichte Der {{SAP}} - {{Die Anfangsjahre}} 1972-1980},
  author = {{SAP SE}},
  url = {https://www.sap.com/germany/about/company/history/1972-1980.html},
  urldate = {2022-03-15}
}

@online{hyperparameter,
  title = {What Is the {{Difference Between}} a {{Parameter}} and a {{Hyperparameter}}?},
  author = {Brownlee, Jason},
  date = {2017-07-25T19:00:17+00:00},
  url = {https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/},
  urldate = {2022-05-26},
  abstract = {It can be confusing when you get started in applied machine learning. There are so many terms to use and many of the terms may not be used consistently. This is especially true if you have come from another field of study that may use some of the same terms as machine learning, but they […]},
  langid = {american},
  file = {/Users/I518232/Zotero/storage/KLAUEDL7/difference-between-a-parameter-and-a-hyperparameter.html}
}

@online{illustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  date = {2018-07-27},
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2022-05-19},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Russian, Spanish, Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic: A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  file = {/Users/I518232/Zotero/storage/9K7DQQ9I/illustrated-transformer.html}
}

@online{internalExternalData,
  title = {3 {{Keys}} to {{Managing Data}} and {{Maximizing Business Intelligence}}},
  author = {Rundle, Dan},
  url = {https://worthwhile.com/insights/2017/02/20/data-business-intelligence/},
  urldate = {2022-04-06},
  abstract = {To maximize your company data and build real business intelligence solutions, you need to understand these 3 dichotomies of data.},
  file = {/Users/I518232/Zotero/storage/DTFJ394S/data-business-intelligence.html}
}

@online{investopediaInvoices,
  title = {Understanding {{Invoices}}},
  author = {Hayes, Adam},
  url = {https://www.investopedia.com/terms/i/invoice.asp},
  urldate = {2022-03-28},
  abstract = {An invoice records itemized transactions and is used for expense management and bookkeeping.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/6IKKUTWY/invoice.html}
}

@online{invoicesPurpose,
  title = {What {{Is}} the {{Purpose}} of an {{Invoice}}? | {{Invoicing Tips}} for {{Small Business}}},
  shorttitle = {What {{Is}} the {{Purpose}} of an {{Invoice}}?},
  url = {https://www.freshbooks.com/hub/invoicing/how-do-invoices-work},
  urldate = {2022-03-28},
  abstract = {The purpose of an invoice is to create a record of sale so businesses can get paid. Learn how professional invoices help small businesses receive payment faster.},
  langid = {american},
  organization = {{FreshBooks}},
  file = {/Users/I518232/Zotero/storage/AT8VLUY8/how-do-invoices-work.html}
}

@online{JuergenMuellerBiography,
  type = {Company Website},
  title = {Juergen {{Mueller}} Biography},
  url = {https://www.sap.com/about/company/leadership/juergen-mueller.html},
  urldate = {2022-03-15},
  langid = {english}
}

@incollection{KanbanAnderson,
  booktitle = {Kanban},
  author = {Anderson, David},
  date = {2011},
  edition = {1st edition},
  abstract = {erstes deutsches Buch zu Kanban Buch vom "Vater" von Kanban geschrieben Kanban ist "hype", gut mit Scrum in Projekten einzusetzen* Übersetzer in der (agilen) Community sehr bekannt(haben die ersten Kanban-Schulungen in D mit David J. Anderson durchgeführt)},
  langid = {eng;ger},
  keywords = {Electronic books}
}

@article{KDD,
  title = {From {{Data Mining}} to {{Knowledge Discovery}} in {{Databases}}},
  author = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
  date = {1996},
  journaltitle = {American Association for Artificial Intelligence},
  volume = {17},
  number = {3},
  entrysubtype = {magazine},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/LEN9UCDZ/Fayyad - From Data Mining to Knowledge Discovery in Databas.pdf}
}

@online{khanRelationshipCosineSimilarity2020,
  title = {Relationship between {{Cosine Similarity}} and {{Euclidean Distance}}.},
  author = {Khan, Tanveer},
  date = {2020-03-10T14:43:52},
  url = {https://medium.com/ai-for-real/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff},
  urldate = {2022-05-23},
  abstract = {Many of us are unaware of a relationship between Cosine Similarity and Euclidean Distance. Knowing this relationship is extremely helpful…},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/A68LN3WH/relationship-between-cosine-similarity-and-euclidean-distance-7e283a277dff.html}
}

@article{kochEInvoicingJourney,
  title = {The {{E-Invoicing Journey}} 2019-2025},
  author = {Koch, Bruno},
  date = {2019-09},
  journaltitle = {Billentis E-Invoicing Journey},
  volume = {4},
  url = {https://www.billentis.com/The_einvoicing_journey_2019-2025.pdf},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/PL4PH5EP/Koch - The E-Invoicing Journey 2019-2025.pdf}
}

@article{kodinariyaReviewDeterminingCluster2013,
  title = {Review on {{Determining}} of {{Cluster}} in {{K-means Clustering}}},
  author = {Kodinariya, Trupti and Makwana, Prashant},
  date = {2013-01-01},
  journaltitle = {International Journal of Advance Research in Computer Science and Management Studies},
  shortjournal = {International Journal of Advance Research in Computer Science and Management Studies},
  volume = {1},
  pages = {90--95}
}

@online{largeDataSetBrownlee,
  title = {7 {{Ways}} to {{Handle Large Data Files}} for {{Machine Learning}}},
  author = {Brownlee, Jason},
  date = {2020-12-10},
  url = {https://machinelearningmastery.com/large-data-files-machine-learning/},
  urldate = {2022-04-28}
}

@online{largeDataSetMedium,
  title = {What to {{Do When Your Data Is Too Big}} for {{Your Memory}}?},
  author = {Metwalli, Sara A.},
  url = {https://towardsdatascience.com/what-to-do-when-your-data-is-too-big-for-your-memory-65c84c600585},
  urldate = {2022-04-13}
}

@article{maierOptimalConstructionKnearest2009,
  title = {Optimal Construction of K-Nearest Neighbor Graphs for Identifying Noisy Clusters},
  author = {Maier, Markus and Hein, Matthias and von Luxburg, Ulrike},
  options = {useprefix=true},
  date = {2009-04},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  volume = {410},
  number = {19},
  pages = {1749--1764},
  issn = {03043975},
  doi = {10.1016/j.tcs.2009.01.009},
  abstract = {We study clustering algorithms based on neighborhood graphs on a random sample of data points. The question we ask is how such a graph should be constructed in order to obtain optimal clustering results. Which type of neighborhood graph should one choose, mutual k-nearest neighbor or symmetric k-nearest neighbor? What is the optimal parameter k? In our setting, clusters are defined as connected components of the t-level set of the underlying probability distribution. Clusters are said to be identified in the neighborhood graph if connected components in the graph correspond to the true underlying clusters. Using techniques from random geometric graph theory, we prove bounds on the probability that clusters are identified successfully, both in a noise-free and in a noisy setting. Those bounds lead to several conclusions. First, k has to be chosen surprisingly high (rather of the order n than of the order log n) to maximize the probability of cluster identification. Secondly, the major difference between the mutual and the symmetric k-nearest neighbor graph occurs when one attempts to detect the most significant cluster only.},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/I518232/Zotero/storage/9KFUTJGF/Maier et al. - 2009 - Optimal construction of k-nearest neighbor graphs .pdf;/Users/I518232/Zotero/storage/U5JQJPVV/0912.html}
}

@online{maklinDBSCANPythonExample2022,
  title = {{{DBSCAN Python Example}}: {{The Optimal Value For Epsilon}} ({{EPS}})},
  shorttitle = {{{DBSCAN Python Example}}},
  author = {Maklin, Cory},
  date = {2022-05-09T23:36:00},
  url = {https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc},
  urldate = {2022-05-17},
  abstract = {Density-Based Spatial Clustering of Applications with Noise, or DBSCAN for short, is an unsupervised machine learning algorithm…},
  langid = {english}
}

@online{maklinDBSCANPythonExample2022a,
  title = {{{DBSCAN Python Example}}: {{The Optimal Value For Epsilon}} ({{EPS}})},
  shorttitle = {{{DBSCAN Python Example}}},
  author = {Maklin, Cory},
  date = {2022-05-09T23:36:00},
  url = {https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc},
  urldate = {2022-05-18},
  abstract = {Density-Based Spatial Clustering of Applications with Noise, or DBSCAN for short, is an unsupervised machine learning algorithm…},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/V9W98M9Y/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3.html}
}

@online{manualInvoiceProcessing,
  title = {It’s Costing How Much? {{The}} Truth about Manual Invoice Processing},
  shorttitle = {It’s Costing How Much?},
  author = {Ball, Daniel},
  date = {2018-05-02},
  url = {https://www.smeweb.com/2018/05/02/costing-much-truth-manual-invoice-processing/},
  urldate = {2022-03-28},
  abstract = {By Daniel Ball, director, Wax Digital The paper-based invoice – the thorn in the side of every finance team but a necessary part of doing business. In},
  langid = {british},
  file = {/Users/I518232/Zotero/storage/HET2JSWY/costing-much-truth-manual-invoice-processing.html}
}

@misc{mBERT,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2020-10-05},
  doi = {10.48550/arXiv.2004.09813},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/I518232/Zotero/storage/QYIB2S72/Reimers and Gurevych - 2020 - Making Monolingual Sentence Embeddings Multilingua.pdf;/Users/I518232/Zotero/storage/AXSD3X3H/2004.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  urldate = {2022-03-24},
  file = {/Users/I518232/Zotero/storage/LKTG7ZQV/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@unpublished{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arxiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2022-03-24},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/I518232/Zotero/storage/7VB2EWHD/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/I518232/Zotero/storage/8XHX2DDH/1301.3781.pdf;/Users/I518232/Zotero/storage/NLS24PG4/1301.html}
}

@online{MLGlossary,
  title = {Machine {{Learning Glossary}}},
  url = {https://developers.google.com/machine-learning/glossary},
  urldate = {2022-05-26},
  abstract = {Compilation of key machine-learning and TensorFlow terms, with beginner-friendly definitions.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/XAV7IEQT/glossary.html}
}

@online{MLM,
  title = {Masked-{{Language Modelling With BERT}}},
  author = {Briggs, James},
  date = {2021-09-02T09:11:03},
  url = {https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c},
  urldate = {2022-05-17},
  abstract = {Transformer models like BERT are incredibly powerful when fine-tuned for specific tasks using masked-language modeling (MLM), we explore how here.},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/I518232/Zotero/storage/XPGYDXR5/masked-language-modelling-with-bert-7d49793e5d2c.html}
}

@online{modernInvoiceManagement,
  title = {Invoice Management for Modern Businesses: {{A}} Guide},
  shorttitle = {Invoice Management for Modern Businesses},
  author = {Whatman, Patrick},
  url = {https://blog.spendesk.com/en/invoice-management},
  urldate = {2022-03-28},
  abstract = {Invoice management hasn't kept up with the times. In most companies, it's still harder than it should be to pay suppliers quickly and easily. This guide will fix that.},
  langid = {british},
  file = {/Users/I518232/Zotero/storage/T39ZUH2P/invoice-management.html}
}

@online{pcaVStsne,
  title = {Dimensionality {{Reduction}} for {{Data Visualization}}: {{PCA}} vs {{TSNE}} vs {{UMAP}} vs {{LDA}}},
  shorttitle = {Dimensionality {{Reduction}} for {{Data Visualization}}},
  author = {Sivarajah, Sivakar},
  date = {2020-12-31T14:12:39},
  url = {https://towardsdatascience.com/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29},
  urldate = {2022-05-21},
  abstract = {Visualising a high-dimensional dataset using: PCA, TSNE, LDA and UMAP},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/4SC9XTD9/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29.html}
}

@book{practicalNLP,
  title = {Practical Natural Language Processing: {{A}} Comprehensive Guide to Building Real-World {{NLP}} Systems},
  author = {Vajjala, S. and Majumder, B. and Gupta, A. and Surana, H.},
  date = {2020},
  publisher = {{O'Reilly Media}},
  url = {https://books.google.de/books?id=G40jywEACAAJ},
  isbn = {978-1-4920-5405-4},
  lccn = {2021275226}
}

@online{pythonDocker,
  title = {Python - {{Official Image}} | {{DockerHub}}},
  url = {https://hub.docker.com/_/python},
  urldate = {2022-04-20}
}

@online{pythonVsR,
  title = {Python vs. {{R}}: {{What}}’s the {{Difference}}?},
  url = {https://www.ibm.com/cloud/blog/python-vs-r},
  urldate = {2022-04-20}
}

@online{rajaramanNeighborSearchHigh,
  title = {Near {{Neighbor Search}} in {{High Dimensional Data}}},
  author = {Rajaraman, Anand},
  url = {https://web.stanford.edu/class/cs345a/slides/04-highdim.pdf},
  urldate = {2022-05-23},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/H7IRCFLL/Rajaraman - Near Neighbor Search in High Dimensional Data (1).pdf}
}

@online{rDocker,
  title = {R-Base - {{Official Image}}},
  url = {https://hub.docker.com/_/r-base},
  urldate = {2022-04-20}
}

@article{robertsonTFIDF,
  title = {Understanding Inverse Document Frequency: {{On}} Theoretical Arguments for {{IDF}}},
  shorttitle = {Understanding Inverse Document Frequency},
  author = {Robertson, Stephen},
  date = {2004},
  journaltitle = {Journal of Documentation},
  volume = {60},
  pages = {2004},
  abstract = {The term weighting function known as IDF was proposed in 1972, and has since been extremely widely used, usually as part of a TF*IDF function. It is often described as a heuristic, and many papers have been written (some based on Shannon’s Information Theory) seeking to establish some theoretical basis for it. Some of these attempts are reviewed, and it is shown that the Information Theory approaches are problematic, but that there are good theoretical justifications of both IDF and TF*IDF in traditional probabilistic model of information retrieval.},
  file = {/Users/I518232/Zotero/storage/XCFQKQIS/Robertson - 2004 - Understanding inverse document frequency On theor.pdf;/Users/I518232/Zotero/storage/7JXSAWQ5/summary.html}
}

@online{rutschmannSAPLeonardo2021,
  title = {The {{Journey}} from {{SAP Leonardo Machine Learning Foundation}} to {{SAP AI Core}} and {{SAP AI Launchpad}}},
  author = {Rutschmann, Daniel},
  date = {2021-10-11},
  url = {https://blogs.sap.com/2021/10/11/the-journey-from-sap-leonardo-machine-learning-foundation-to-sap-ai-core-and-sap-ai-launchpad/},
  urldate = {2022-03-16}
}

@online{RVsPythonJohnson,
  title = {R {{Vs Python}}: {{What}}’s the {{Difference}}?},
  url = {https://www.guru99.com/r-vs-python.html}
}

@online{SAPSEExecutive,
  title = {{{SAP SE Executive Board}}},
  url = {https://www.sap.com/investors/en/governance/executive-board.html},
  organization = {{SAP SE Corporate Governance}}
}

@online{scaleAI,
  title = {{{ScaleAI}}},
  url = {https://scale.com/document-ai}
}

@online{ScaleDocumentAI,
  title = {Scale {{Document AI}}: {{Template-Free}}, {{High-Quality Extraction}}},
  url = {https://scale.com/document-ai},
  urldate = {2022-05-09},
  organization = {{Scale Document AI: Template-Free, High-Quality Extraction}}
}

@online{schmitzLeonardo,
  title = {Was ist SAP Leonardo?},
  author = {Schmitz, Andreas},
  date = {2017-07-11},
  url = {https://news.sap.com/germany/2017/07/was-ist-sap-leonardo-iot/},
  urldate = {2022-03-16},
  langid = {german}
}

@online{schopfParallelInferenceHuggingFace2022,
  title = {Parallel {{Inference}} of {{HuggingFace Transformers}} on {{CPUs}}},
  author = {Schopf, Tim},
  date = {2022-02-22T10:13:51},
  url = {https://towardsdatascience.com/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23},
  urldate = {2022-05-22},
  abstract = {An introduction to multiprocessing predictions of large machine learning and deep learning models},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/Z2MYKL2E/parallel-inference-of-huggingface-transformers-on-cpus-4487c28abe23.html}
}

@incollection{schubertTriangleInequalityCosine2021,
  title = {A {{Triangle Inequality}} for {{Cosine Similarity}}},
  booktitle = {Similarity {{Search}} and {{Applications}}},
  author = {Schubert, Erich},
  date = {2021},
  volume = {2021},
  pages = {32--44},
  url = {http://arxiv.org/abs/2107.04071},
  urldate = {2022-05-23},
  abstract = {Similarity search is a fundamental problem for many data analysis techniques. Many efficient search techniques rely on the triangle inequality of metrics, which allows pruning parts of the search space based on transitive bounds on distances. Recently, Cosine similarity has become a popular alternative choice to the standard Euclidean metric, in particular in the context of textual data and neural network embeddings. Unfortunately, Cosine similarity is not metric and does not satisfy the standard triangle inequality. Instead, many search techniques for Cosine rely on approximation techniques such as locality sensitive hashing. In this paper, we derive a triangle inequality for Cosine similarity that is suitable for efficient similarity search with many standard search structures (such as the VP-tree, Cover-tree, and M-tree); show that this bound is tight and discuss fast approximations for it. We hope that this spurs new research on accelerating exact similarity search for cosine similarity, and possible other similarity measures beyond the existing work for distance metrics.},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/I518232/Zotero/storage/8XDXMB7P/Schubert - 2021 - A Triangle Inequality for Cosine Similarity.pdf;/Users/I518232/Zotero/storage/JLNPQ95R/2107.html}
}

@online{SCRUMSolo,
  title = {Scrum and the {{Solo Dev}}},
  author = {Gant, Michael},
  date = {2019-04-15},
  url = {https://medium.com/@jmgant.cleareyeconsulting/scrum-and-the-solo-dev-fb8e810ed42b},
  urldate = {2022-03-29},
  abstract = {Can a solo developer follow a Scrum process? Google it, you’ll get a few different answers. Here’s mine.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/X4YGSRA7/scrum-and-the-solo-dev-fb8e810ed42b.html}
}

@article{sculleyWebscaleKmeansClustering2010,
  title = {Web-Scale k-Means Clustering},
  author = {Sculley, D.},
  date = {2010},
  journaltitle = {International conference on World wide web},
  volume = {19},
  pages = {1177},
  doi = {10.1145/1772690.1772862},
  url = {http://portal.acm.org/citation.cfm?doid=1772690.1772862},
  urldate = {2022-05-18},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/TD6DTQVJ/Sculley - 2010 - Web-scale k-means clustering.pdf}
}

@misc{sentenceBERT,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  date = {2019-08-27},
  doi = {10.48550/arxiv.1908.10084},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textasciitilde 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/I518232/Zotero/storage/33FV7TFL/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/I518232/Zotero/storage/C59JTX9C/1908.html}
}

@article{sklearn,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{sklearn,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@online{SklearnClusterDBSCAN,
  title = {Sklearn.Cluster.{{DBSCAN}}},
  url = {https://scikit-learn/stable/modules/generated/sklearn.cluster.DBSCAN.html},
  urldate = {2022-05-18},
  abstract = {Examples using sklearn.cluster.DBSCAN: Comparing different clustering algorithms on toy datasets Comparing different clustering algorithms on toy datasets, Demo of DBSCAN clustering algorithm Demo ...},
  langid = {english},
  organization = {{scikit-learn}},
  file = {/Users/I518232/Zotero/storage/Q7DKI5QP/sklearn.cluster.DBSCAN.html}
}

@online{srivastavaDataMining,
  title = {Difference {{Between Data Science}} and {{Data Mining}}},
  author = {Srivastava, Astitva},
  date = {2020-07-25T15:35:55},
  url = {https://medium.com/swlh/difference-between-data-science-and-data-mining-37104b1c6a61},
  urldate = {2022-04-06},
  abstract = {Sometimes it might confusing to some people to distinguish between Data Science and Data Mining, so after reading this article it will…},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/N2CZY6SV/difference-between-data-science-and-data-mining-37104b1c6a61.html}
}

@online{structuredAndUnstructuredData,
  title = {Structured vs {{Unstructured Data}}},
  shorttitle = {Structured vs {{Unstructured Data}} 101},
  author = {Taylor, Christine},
  date = {2021-05-21T15:00:00+00:00},
  url = {https://www.datamation.com/big-data/structured-vs-unstructured-data/},
  urldate = {2022-03-28},
  abstract = {Structured Data is organized \& often formatted, and Unstructured Data is raw data of various types. Learn key differences \& how each is used.},
  langid = {american},
  file = {/Users/I518232/Zotero/storage/8I9BQTGZ/structured-vs-unstructured-data.html}
}

@misc{transformersAttention,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-05-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/I518232/Zotero/storage/9I2FHG3P/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/I518232/Zotero/storage/VJKYV9JW/1706.html}
}

@article{turing1950,
  title = {I.—{{COMPUTING MACHINERY AND IN}}℡{{LIGENCE}}},
  author = {TURING, A. M.},
  date = {1950-10},
  journaltitle = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  annotation = {\_eprint: https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf}
}

@online{whyExternalData,
  title = {Why External Data Should Be Part of Your Data Strategy},
  author = {Brown, Sara},
  url = {https://mitsloan.mit.edu/ideas-made-to-matter/why-external-data-should-be-part-your-data-strategy},
  urldate = {2022-04-06},
  abstract = {Third-party data is out there to find and use. To make external data work for your firm, vet it carefully and make it part of a centralized data strategy.},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/2EIVF7K3/why-external-data-should-be-part-your-data-strategy.html}
}

@inproceedings{wirthCRISPDMStandardProcess2000,
  title = {{{CRISP-DM}}: {{Towards}} a Standard Process Model for Data Mining},
  booktitle = {Proceedings of the 4th International Conference on the Practical Applications of Knowledge Discovery and Data Mining},
  author = {Wirth, Rüdiger and Hipp, Jochen},
  date = {2000},
  volume = {1},
  pages = {29--40},
  publisher = {{Manchester}},
  file = {/Users/I518232/Zotero/storage/GNBBS264/Wirth and Hipp - CRISP-DM Towards a Standard Process Model for Dat.pdf}
}

@online{yildirimTwoChallengesKMeans2020,
  title = {Two {{Challenges}} of {{K-Means Clustering}}},
  author = {Yıldırım, Soner},
  date = {2020-04-06T22:12:31},
  url = {https://towardsdatascience.com/two-challenges-of-k-means-clustering-72e90bdeb0da},
  urldate = {2022-05-24},
  abstract = {How to “wisely” pick k and initial centroids},
  langid = {english},
  file = {/Users/I518232/Zotero/storage/NMHKTQAU/two-challenges-of-k-means-clustering-72e90bdeb0da.html}
}

@article{zotero-1,
  type = {article}
}

@book{zotero-undefined,
  type = {book}
}

@online{zotero-undefineda,
  url = {https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c}
}


